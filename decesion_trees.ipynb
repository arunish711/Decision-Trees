{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are calculating the gain ratio.\n",
    "\n",
    "def calculate_gain_ratio(total,total_x1,total_x2,entropy,entropy_x1,entropy_x2):\n",
    "    info_gain=entropy-((total_x1/total)*entropy_x1+(total_x2/total)*entropy_x2)\n",
    "    sum_1=0\n",
    "    sum_2=0;\n",
    "    if(total_x1!=0):\n",
    "        cal_1=total/total_x1\n",
    "        sum_1=(total_x1/total)*(m.log(cal_1,2))\n",
    "    if(total_x2!=0):\n",
    "        cal_2=total/total_x2\n",
    "        sum_2=(total_x2/total)*(m.log(cal_2,2))\n",
    "    split_info=sum_1+sum_2\n",
    "    gain_ratio=info_gain/split_info\n",
    "    return gain_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are calculating the entropy .\n",
    "\n",
    "def calculate_entropy(x1_0,x1_1,x2_0,x2_1):\n",
    "    total_1=x1_1+x2_1\n",
    "    total_0=x2_0+x1_0\n",
    "    total=total_1+total_0\n",
    "    sum_1=0\n",
    "    sum_0=0\n",
    "    if(total_1!=0):\n",
    "        num_1=total/total_1\n",
    "        sum_1=((total_1/total))*(m.log(num_1,2))\n",
    "    if(total_0!=0):\n",
    "        num_0=total/total_0\n",
    "        sum_0=((total_0/total))*(m.log(num_0,2))\n",
    "    entropy=sum_1+sum_0\n",
    "    \n",
    "    sum_1=0\n",
    "    sum_0=0\n",
    "    total_x1=x1_0+x1_1\n",
    "    if(x1_1!=0):\n",
    "        num_x1_1=total_x1/x1_1\n",
    "        sum_1=((x1_1/total_x1))*(m.log(num_x1_1,2))\n",
    "    if(x1_0!=0):\n",
    "        num_x1_0=total_x1/x1_0\n",
    "        sum_0=((x1_0/total_x1))*(m.log(num_x1_0,2))\n",
    "    entropy_x1=sum_1+sum_0\n",
    "    \n",
    "    sum_1=0\n",
    "    sum_2=0\n",
    "    total_x2=x2_0+x2_1\n",
    "    if(x2_1!=0):\n",
    "        num_x2_1=total_x2/x2_1\n",
    "        sum_1=((x2_1/total_x2))*(m.log(num_x2_1,2))\n",
    "    if(x2_0!=0):\n",
    "        num_x2_0=total_x2/x2_0\n",
    "        sum_0=((x2_0/total_x2))*(m.log(num_x2_0,2))\n",
    "    entropy_x2=sum_1+sum_0\n",
    "    \n",
    "    gain_ratio=calculate_gain_ratio(total,total_x1,total_x2,entropy,entropy_x1,entropy_x2)\n",
    "    \n",
    "    return gain_ratio,entropy,total_1,total_0\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are just storing the count of various zero and ones as output on splitting.\n",
    "\n",
    "def gain_(f,x_train,y_train,wall,df):\n",
    "    x1=df[df[f]<wall]\n",
    "    x2=df[df[f]>=wall]\n",
    "    l1=list(x1.iloc[:,4])\n",
    "    l2=list(x2.iloc[:,4])\n",
    "    count_x1_0=l1.count(0)\n",
    "    count_x1_1=l1.count(1)\n",
    "    count_x2_0=l2.count(0)\n",
    "    count_x2_1=l2.count(1)\n",
    "    return calculate_entropy(count_x1_0,count_x1_1,count_x2_0,count_x2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the gain ratio for the continuous data.\n",
    "\n",
    "def gain(f,x_train,y_train,df):\n",
    "    s1=x_train[f] \n",
    "    s1=set(s1)  # Storing the unique values in the set s1.  \n",
    "    list1=list(s1) # Converting into list.\n",
    "    list1.sort()   # Sorting the list.\n",
    "    max_gain=-5\n",
    "    for i in range(len(list1)-1):\n",
    "        wall=(list1[i]+list1[i+1])/2 # Taking the arithmetic mean between the two numbers.\n",
    "        ans=gain_(f,x_train,y_train,wall,df) # Calculting the entropy and gain ratio using the gain function of the arithmetic mean.\n",
    "        ans=list(ans)\n",
    "        if(max_gain<=ans[0]):\n",
    "            max_gain=ans[0]\n",
    "            sol=list(ans)\n",
    "            sol.append(wall) # Storing the best arithmetic mean.\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are choosing the best feature to split and then we are splitting our decesion tree. \n",
    "\n",
    "def decesion_tree(x_train,y_train,features,df,level):\n",
    "    \n",
    "    max_gain1=-4\n",
    "    \n",
    "    if len(set(y_train))==1: # If the classes are pure. ( Base case of the recursive function (decesion_tree))\n",
    "        for f in features:\n",
    "            ans_=gain(f,x_train,y_train,df) # Checking each and every feature and choosing the best among them.\n",
    "            ans_=list(ans_)\n",
    "            if max_gain1<=ans_[0]:\n",
    "                max_gain1=ans_[0]\n",
    "                sol_=list(ans_)\n",
    "                sol_.append(f)\n",
    "    \n",
    "        feat_=sol_[5]\n",
    "        features.remove(sol_[5]) # Removing  the feature we have chosen to split on the basis of it.\n",
    "        \n",
    "        print(\"Level \",level)\n",
    "        print(\"Count of zero \",sol_[3])\n",
    "        print(\"Count of one \",sol_[2])\n",
    "        print(\"Current Entropy \",sol_[1])\n",
    "        print(\"Splitting on the feature \",feat_,\" with Gain Ratio \",sol_[0])\n",
    "        print()\n",
    "    \n",
    "        return \n",
    "    \n",
    "    if(len(features)==0): # Here we are returing when no feature is left to split.\n",
    "        return\n",
    "\n",
    "    \n",
    "    # BASE CASES ENDS.\n",
    "    \n",
    "    for i in features:\n",
    "        ans1=gain(i,x_train,y_train,df) # Checking each and every feature and choosing the best among them.\n",
    "        ans1=list(ans1)\n",
    "        if max_gain1<=ans1[0]:\n",
    "            max_gain1=ans1[0]\n",
    "            sol1=list(ans1)\n",
    "            sol1.append(i)\n",
    "    wall=sol1[4]\n",
    "    feat=sol1[5]\n",
    "    df1=df[df[feat]<wall]\n",
    "    df2=df[df[feat]>=wall]\n",
    "    \n",
    "    x1_train=df1.iloc[:,0:3]\n",
    "    y1_train=df1.iloc[:,4]\n",
    "    x2_train=df2.iloc[:,0:3]\n",
    "    y2_train=df2.iloc[:,4]\n",
    "\n",
    "    features.remove(feat) # Removing  the feature we have chosen to split on the basis of it.\n",
    "    \n",
    "    decesion_tree(x1_train,y1_train,features,df1,level+1) # We are recursively calling on the splitted data.\n",
    "    decesion_tree(x2_train,y2_train,features,df2,level+1) # We are recursively calling on the splitted data.\n",
    "    \n",
    "    print(\"Level \",level)\n",
    "    print(\"Count of zero \",sol1[3])\n",
    "    print(\"Count of one \",sol1[2])\n",
    "    print(\"Current Entropy \",sol1[1])\n",
    "    print(\"Splitting on the feature \",feat,\" with Gain Ratio \",sol1[0])\n",
    "    print()\n",
    "    \n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are converting the string data into int data type.\n",
    "\n",
    "def f(s):\n",
    "    if(s==\"Iris-setosa\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")  #Extracting iris data\n",
    "\n",
    "df=iris.copy() # Creating df as a panda type and storing the copy of iris data in it.\n",
    "\n",
    "df.columns=[\"sl\",\"sw\",\"pl\",\"pw\",\"flower_type\"] # Creating column header of every column.\n",
    "\n",
    "df[\"flower_type\"]=df.flower_type.apply(f) # Changing string data into number so that it can be made easy to use the data.\n",
    "\n",
    "features=df.columns # Storing the feature_names in the feature.\n",
    "\n",
    "features=list(features)\n",
    "\n",
    "features.remove(\"flower_type\") # Removing flower_type as a feature as it is not a feature_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level  1\n",
      "Count of zero  49\n",
      "Count of one  0\n",
      "Current Entropy  0.0\n",
      "Splitting on the feature  pl  with Gain Ratio  0.0\n",
      "\n",
      "Level  1\n",
      "Count of zero  0\n",
      "Count of one  100\n",
      "Current Entropy  0.0\n",
      "Splitting on the feature  sw  with Gain Ratio  0.0\n",
      "\n",
      "Level  0\n",
      "Count of zero  49\n",
      "Count of one  100\n",
      "Current Entropy  0.913756430937882\n",
      "Splitting on the feature  pw  with Gain Ratio  1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train=df.iloc[:,0:df.shape[1]-1] # Extracting  input training  data.\n",
    "\n",
    "y_train=df.iloc[:,df.shape[1]-1] # Extracting  output training data.\n",
    "\n",
    "decesion_tree(x_train,y_train,features,df,0) # Printing the information regarding the decesion tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
